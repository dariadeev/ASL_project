# -*- coding: utf-8 -*-
"""Copy of ASL Wandb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13Kt895aq4ZMQsnfYNpQ9k0uF4o44T3v3
"""

# # %%1
# # Mount Google Drive in Colab
# from google.colab import drive
# drive.mount('/content/drive')

# print("Google Drive mounted at /content/drive")

# %%2
import os

# Create the project folder on Google Drive if it doesn't already exist
# Replace 'ASL_Project_WNB' with your preferred root folder name for the project
project_root_drive = '/content/drive/My Drive/ASL_Project_WNB'
os.makedirs(project_root_drive, exist_ok=True)

# Define paths for checkpoints and W&B logs inside the project folder on Drive
checkpoints_dir_drive = os.path.join(project_root_drive, 'checkpoints')
wandb_logs_dir_drive = os.path.join(project_root_drive, 'wandb_logs')

os.makedirs(checkpoints_dir_drive, exist_ok=True)
os.makedirs(wandb_logs_dir_drive, exist_ok=True)

print(f"Project folder on Drive: {project_root_drive}")
print(f"Checkpoints saved in: {checkpoints_dir_drive}")
print(f"W&B logs saved in: {wandb_logs_dir_drive}")

# %%3
# Install required libraries
!pip install -qqq pytorch-lightning wandb torchmetrics scikit-learn torchvision

# Essential imports
import os, glob, time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from torchvision.datasets import ImageFolder
import pytorch_lightning as pl  # Import PyTorch Lightning as pl
from pytorch_lightning import LightningModule, Trainer, Callback
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger
from torchmetrics import Accuracy
from sklearn.model_selection import train_test_split
import shutil
import zipfile
from tqdm.notebook import tqdm  # For a nice progress bar in notebooks
import wandb

print("Libraries installed and imported.")

# # %%
# # Cell 4: Download and Unzip the ASL dataset
# # IMPORTANT: Make sure kaggle.json is already uploaded to /content/

# !mkdir -p ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

# print("Kaggle setup complete. Downloading dataset...")
# !kaggle datasets download -d grassknoted/asl-alphabet -p /content/

# # --- KEY MODIFICATION HERE ---
# # We will extract directly into '/content/data'
# # Make sure DEST_ROOT is defined earlier if used elsewhere.
# # Or define it here if this is the only usage.
# extract_path_for_unzip = '/content/data/'  # <--- NEW EXTRACTION PATH

# # Create the destination folder if it doesn't already exist
# # Normally created by Cell 5, but we ensure it here just in case.
# import os  # Make sure os is imported
# os.makedirs(extract_path_for_unzip, exist_ok=True)

# zip_file_path = '/content/asl_data.zip'

# print(f"Unzipping {zip_file_path} to {extract_path_for_unzip}...")
# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
#     # Extract everything. The folder 'asl_alphabet_train' will be created directly under '/content/data/'
#     zip_ref.extractall(extract_path_for_unzip)
# print("Unzipping complete.")

# # --- POST-UNZIP CHECKS ADJUSTED ---
# print("\n--- Verifying extracted files ---")
# !ls -lh {zip_file_path}  # Should show the size of the zip file
# # Now the folder 'asl_alphabet_train' should be directly under /content/data/
# !ls -lh {extract_path_for_unzip}  # Should show 'asl_alphabet_train' and 'asl_alphabet_test'
# !ls -lh {os.path.join(extract_path_for_unzip, 'train', 'A')}  # Should list JPG images in class 'A'
# !du -sh {os.path.join(extract_path_for_unzip, 'train')}  # Should show total size of training folder

# %%
# Cell 4: Downloading and Extracting the ASL Dataset
# IMPORTANT: Make sure kaggle.json is already uploaded in /content/

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

print("Kaggle configuration complete. Downloading the dataset...")
!kaggle datasets download -d grassknoted/asl-alphabet -p /content/

zip_file_path = '/content/asl-alphabet.zip'

# --- KEY CHANGE HERE ---
# We will extract directly into '/content/data'
# Make sure DEST_ROOT is defined earlier if you're using it elsewhere.
# Or define it here if this is the only usage.
extract_path_for_unzip = '/content/data/'  # <--- NEW EXTRACTION PATH

# Create the destination folder if it doesn't already exist
# It’s normally created in cell 5, but we can do it here just to be sure.
import os  # Make sure os is imported
os.makedirs(extract_path_for_unzip, exist_ok=True)

print(f"Extracting {zip_file_path} to {extract_path_for_unzip}...")
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    # Extract everything. The folder 'asl_alphabet_train' will be created directly under '/content/data/'
    zip_ref.extractall(extract_path_for_unzip)
print("Extraction complete.")

# --- POST-EXTRACTION CHECKS ADJUSTED ---
print("\n--- Verifying extracted files ---")
!ls -lh {zip_file_path}  # Should show the size of the zip file
# Now, the folder asl_alphabet_train should be directly under /content/data/
!ls -lh {extract_path_for_unzip}  # Should show 'asl_alphabet_train' and 'asl_alphabet_test'
!ls -lh {os.path.join(extract_path_for_unzip, 'asl_alphabet_train', 'A')}  # Should show JPG images
!du -sh {os.path.join(extract_path_for_unzip, 'asl_alphabet_train')}  # Should show the size of the training folder

# %%
# Cell 5: Split the ASL dataset into train/val/test sets

# 1. Define paths
# This is the path where the zip was extracted and where we expect to find the class folders
SOURCE_DIR_RAW = '/content/data/asl_alphabet_train'

# This is the target root directory where train/val/test folders will be created
DEST_ROOT = '/content/data'

TRAIN_RATIO = 0.8
VAL_RATIO = 0.1
TEST_RATIO = 0.1

assert TRAIN_RATIO + VAL_RATIO + TEST_RATIO == 1.0, "Train, validation, and test ratios must add up to 1.0"

print(f"\n--- Checking and flattening dataset folder structure ---")

# 2. Check for nested folder structure and fix if necessary
nested_folder_path = os.path.join(SOURCE_DIR_RAW, 'asl_alphabet_train')

if os.path.exists(nested_folder_path) and os.path.isdir(nested_folder_path):
    print(f"Nested folder detected: {nested_folder_path}")

    # Create a temporary flat folder to safely move contents
    temp_flat_dir = '/content/data/asl_alphabet_train_temp_flat'
    os.makedirs(temp_flat_dir, exist_ok=True)

    print(f"Moving contents from '{nested_folder_path}' to '{temp_flat_dir}'...")
    for item_name in os.listdir(nested_folder_path):
        shutil.move(os.path.join(nested_folder_path, item_name), temp_flat_dir)

    # Delete the old nested structure
    shutil.rmtree(SOURCE_DIR_RAW)  # Removes the original root that held the nested folder

    # Rename the flat folder to become the new expected root
    os.rename(temp_flat_dir, SOURCE_DIR_RAW)
    print(f"Folder structure flattened. Source is now: {SOURCE_DIR_RAW}")
else:
    print(f"No nested folder detected. Source folder is already flat: {SOURCE_DIR_RAW}")

# 3. Create destination folders for train, val, and test
os.makedirs(os.path.join(DEST_ROOT, 'train'), exist_ok=True)
os.makedirs(os.path.join(DEST_ROOT, 'val'), exist_ok=True)
os.makedirs(os.path.join(DEST_ROOT, 'test'), exist_ok=True)

print("\nStarting dataset split...")

# 4. Perform splitting logic
SOURCE_FOR_SPLIT = SOURCE_DIR_RAW

class_names = sorted(os.listdir(SOURCE_FOR_SPLIT))
if not class_names:
    print(f"FATAL ERROR: No class folders found in {SOURCE_FOR_SPLIT}. Exiting.")
else:
    print(f"Found {len(class_names)} classes in {SOURCE_FOR_SPLIT}.")
    for class_name in tqdm(class_names, desc="Splitting classes"):
        class_path = os.path.join(SOURCE_FOR_SPLIT, class_name)
        if not os.path.isdir(class_path):
            print(f"Skipping {class_path} — not a folder.")
            continue

        # Destination directories for each class
        train_class_dir = os.path.join(DEST_ROOT, 'train', class_name)
        val_class_dir = os.path.join(DEST_ROOT, 'val', class_name)
        test_class_dir = os.path.join(DEST_ROOT, 'test', class_name)

        os.makedirs(train_class_dir, exist_ok=True)
        os.makedirs(val_class_dir, exist_ok=True)
        os.makedirs(test_class_dir, exist_ok=True)

        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

        if not images:
            print(f"Warning: No images found for class {class_name} in {class_path}. Skipping.")
            continue

        if class_name == 'A':
            print(f"  Class '{class_name}': Found {len(images)} images.")

        # Split: train / (val + test)
        train_images, temp_images = train_test_split(images, test_size=(VAL_RATIO + TEST_RATIO), random_state=42, shuffle=True)
        val_images, test_images = train_test_split(temp_images, test_size=(TEST_RATIO / (VAL_RATIO + TEST_RATIO)), random_state=42, shuffle=True)

        # Copy images to respective folders
        for img in train_images:
            shutil.copy(os.path.join(class_path, img), os.path.join(train_class_dir, img))

        for img in val_images:
            shutil.copy(os.path.join(class_path, img), os.path.join(val_class_dir, img))

        for img in test_images:
            shutil.copy(os.path.join(class_path, img), os.path.join(test_class_dir, img))

        if class_name == 'A':
            print(f"  Copied for class '{class_name}': {len(train_images)} (train), {len(val_images)} (val), {len(test_images)} (test).")

print("✅ Dataset splitting completed.")
print(f"Ready-to-use dataset is located in: {DEST_ROOT}")

# --- FINAL VERIFICATION ---
print("\n--- Final check of train/val/test directories ---")
!ls -lh /content/data/train/A        # Should list image files
!du -sh /content/data/train/         # Should show total size (e.g., >200M)
!find /content/data -type f -name "*.jpg" | wc -l  # Total image count

from pathlib import Path

splits = ['train', 'val', 'test']
root = Path("data")

for split in splits:
    split_dir = root / split
    print(f"\n📁 Checking '{split_dir}':")
    if not split_dir.exists():
        print("❌ Not found")
        continue
    subdirs = sorted([d.name for d in split_dir.iterdir() if d.is_dir()])
    print(f"✅ Found {len(subdirs)} classes: {subdirs[:5]}{' ...' if len(subdirs) > 5 else ''}")

from torchvision.datasets import ImageFolder
from torchvision import transforms
from PIL import Image

# Simple transform for debug
t = transforms.ToTensor()
ds = ImageFolder("data/train", transform=t)
print(f"✅ Loaded {len(ds)} images across {len(ds.classes)} classes: {ds.classes[:5]}...")

# Load one image
img, label = ds[0]
print(f"First image shape: {img.shape}, label: {ds.classes[label]}")

import os
import random
from pathlib import Path

def split_dataset_symlinks(src_dir, val_dir, test_dir, val_ratio=0.1, test_ratio=0.1):
    random.seed(42)
    src_dir = Path(src_dir)
    val_dir = Path(val_dir)
    test_dir = Path(test_dir)

    for split_dir in [val_dir, test_dir]:
        split_dir.mkdir(parents=True, exist_ok=True)

    classes = [d.name for d in src_dir.iterdir() if d.is_dir()]
    print(f"📁 Found {len(classes)} classes.")

    for cls in classes:
        cls_src = src_dir / cls
        images = sorted([f for f in cls_src.glob("*") if f.suffix in ['.jpg', '.jpeg', '.png']])
        random.shuffle(images)

        n_val = int(len(images) * val_ratio)
        n_test = int(len(images) * test_ratio)

        val_images = images[:n_val]
        test_images = images[n_val:n_val + n_test]

        for split_images, split_dir in [(val_images, val_dir), (test_images, test_dir)]:
            cls_dst = split_dir / cls
            cls_dst.mkdir(parents=True, exist_ok=True)
            for img in split_images:
                link_path = cls_dst / img.name
                if not link_path.exists():
                    os.symlink(os.path.abspath(img), link_path)

        print(f"✅ {cls}: {len(images)} total → {n_val} val, {n_test} test")

split_dataset_symlinks(
    src_dir="data/train",
    val_dir="data/val",
    test_dir="data/test",
    val_ratio=0.1,
    test_ratio=0.1
)

# %%66666666
# Definition of ASLDataModule
class ASLDataModule(pl.LightningDataModule):
    def __init__(self, data_dir: str, batch_size: int = 64, num_workers: int = 4):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]
        self.num_classes = 0  # Will be set in setup()

    def setup(self, stage=None):
        train_transforms = transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(self.mean, self.std),
            transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),
        ])

        val_transforms = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(self.mean, self.std),
        ])

        self.train_dataset = ImageFolder(os.path.join(self.data_dir, 'train'), transform=train_transforms)
        self.val_dataset = ImageFolder(os.path.join(self.data_dir, 'val'), transform=val_transforms)
        self.test_dataset = ImageFolder(os.path.join(self.data_dir, 'test'), transform=val_transforms)

        self.num_classes = len(self.train_dataset.classes)  # Dynamically detect number of classes

        print(f"Datasets loaded — Train: {len(self.train_dataset)} images, "
              f"Val: {len(self.val_dataset)} images, Test: {len(self.test_dataset)} images.")
        print(f"Detected classes: {self.train_dataset.classes}")

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size,
                          shuffle=True, num_workers=self.num_workers, pin_memory=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size,
                          shuffle=False, num_workers=self.num_workers, pin_memory=True)

    def test_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.batch_size,
                          shuffle=False, num_workers=self.num_workers, pin_memory=True)

# %%7
# Make sure all necessary imports are defined earlier in the script.
import os, time, glob, torch
import torch.nn as nn
import torch.optim as optim
import pytorch_lightning as pl
from pytorch_lightning import LightningModule, Trainer, Callback
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import WandbLogger
from torchvision import models
from torchmetrics.classification import MulticlassAccuracy
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import wandb

class WNBModule(LightningModule):
    def __init__(self, project_name: str, experiment_name: str,
                 checkpoint_dir: str,
                 wandb_save_dir: str,
                 num_classes: int = 29,
                 learning_rate: float = 1e-3,
                 unfreeze_epoch: int = 5,
                 time_interval: int = 15):
        super().__init__()
        self.save_hyperparameters()

        self.project_name = project_name
        self.experiment_name = experiment_name
        self.checkpoint_dir = checkpoint_dir
        self.wandb_save_dir = wandb_save_dir
        self.time_interval = time_interval

        self.num_classes = num_classes
        self.learning_rate = learning_rate
        self.unfreeze_epoch = unfreeze_epoch

        os.makedirs(self.checkpoint_dir, exist_ok=True)
        os.makedirs(self.wandb_save_dir, exist_ok=True)

        self.wandb_logger = WandbLogger(
            project=self.project_name,
            name=self.experiment_name,
            save_dir=self.wandb_save_dir,
            resume="allow"
        )
        print(f"W&B Logger initialized: Project - {self.wandb_logger.experiment.project_name}, Run - {self.wandb_logger.experiment.name}")

        # Model Definition: ResNet18
        self.backbone = models.resnet18(pretrained=True)
        num_ftrs = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(num_ftrs, self.num_classes)

        # Freeze all layers except the final FC layer
        for param in self.backbone.parameters():
            param.requires_grad = False
        for param in self.backbone.fc.parameters():
            param.requires_grad = True

        # Metrics
        self.train_acc = MulticlassAccuracy(num_classes=num_classes)
        self.val_acc = MulticlassAccuracy(num_classes=num_classes)
        self.test_acc = MulticlassAccuracy(num_classes=num_classes)

        # Loss
        self.criterion = nn.CrossEntropyLoss()

        # Callbacks
        self.callbacks_list = []

        # Epoch-based checkpoint
        self.ckpt_epoch = ModelCheckpoint(
            dirpath=self.checkpoint_dir,
            filename="epoch-{epoch:02d}-{val_acc:.3f}",
            save_top_k=1,
            monitor='val_acc',
            mode='max',
            save_last=True
        )
        self.callbacks_list.append(self.ckpt_epoch)

        # Time-based checkpoint
        class TimeBasedCheckpoint(Callback):
            def __init__(self, save_every_minutes, dirpath_base):
                super().__init__()
                self.save_every_sec = save_every_minutes * 60
                self.dirpath_base = dirpath_base
                self.last_save = time.time()

            def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
                current_time = time.time()
                if current_time - self.last_save >= self.save_every_sec:
                    ckpt_filename = f"time_ckpt_epoch{trainer.current_epoch:02d}_step{trainer.global_step}_{int(current_time)}.ckpt"
                    ckpt_path = os.path.join(self.dirpath_base, ckpt_filename)
                    trainer.save_checkpoint(ckpt_path)
                    pl_module.print(f"Saved time-based checkpoint: {ckpt_path}")
                    self.last_save = current_time

        self.ckpt_time = TimeBasedCheckpoint(self.time_interval, self.checkpoint_dir)
        self.callbacks_list.append(self.ckpt_time)

        # Early stopping
        self.early_stop_callback = EarlyStopping(
            monitor='val_acc',
            patience=7,
            mode='max',
            verbose=True
        )
        self.callbacks_list.append(self.early_stop_callback)

    # Forward pass
    def forward(self, x):
        return self.backbone(x)

    # Training step
    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        self.train_acc(logits, y)
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_acc', self.train_acc, on_step=True, on_epoch=True, prog_bar=True)
        return loss

    # Validation step
    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        self.val_acc(logits, y)
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True)
        return loss

    # Test step
    def test_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)
        self.test_acc(logits, y)
        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log('test_acc', self.test_acc, on_step=False, on_epoch=True, prog_bar=True)
        return loss

    # Optimizer configuration
    def configure_optimizers(self):
        return optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), lr=self.learning_rate)

    # Unfreeze backbone at specified epoch
    def on_train_epoch_start(self):
        if self.current_epoch == self.unfreeze_epoch:
            print(f"--- Unfreezing backbone at epoch {self.unfreeze_epoch}. Starting fine-tuning. ---")
            for param in self.backbone.parameters():
                param.requires_grad = True

    # Custom training launch method
    def train_model(self, datamodule: pl.LightningDataModule, max_epochs: int, gpus: int = 1, **trainer_kwargs):
        all_ckpts = glob.glob(os.path.join(self.checkpoint_dir, "*.ckpt"))
        ckpt_path = max(all_ckpts, key=os.path.getmtime) if all_ckpts else None

        if ckpt_path:
            print(f"Resuming training from checkpoint: {ckpt_path}")
        else:
            print("No checkpoint found, starting new training.")

        trainer = Trainer(
            logger=self.wandb_logger,
            callbacks=self.callbacks_list,
            max_epochs=max_epochs,
            accelerator="gpu" if gpus > 0 else "cpu",
            devices=gpus if gpus > 0 else 1,
            precision=16,
            log_every_n_steps=50,
            deterministic=True,
            **trainer_kwargs
        )

        print("\n--- Training started ---")
        trainer.fit(self, datamodule=datamodule, ckpt_path=ckpt_path)
        print("--- Training completed ---")

        print("\n--- Testing the best model ---")
        trainer.test(datamodule=datamodule, ckpt_path='best')
        print("--- Test completed ---")

        if wandb.run is not None:
            wandb.finish()

# %%6
# Definition of ASLDataModule
class ASLDataModule(pl.LightningDataModule):
    def __init__(self, data_dir: str, batch_size: int = 64, num_workers: int = 4):
        super().__init__()
        self.data_dir = data_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]
        self.num_classes = 0  # Will be updated in setup

    def setup(self, stage=None):
        train_transforms = transforms.Compose([
            transforms.RandomResizedCrop(224),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
            transforms.ToTensor(),
            transforms.Normalize(self.mean, self.std),
            transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3)),
        ])

        val_transforms = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(self.mean, self.std),
        ])

        # Fixed: Point ImageFolder to the root directory data_dir
        # ImageFolder will find class subfolders in 'train', 'val', 'test'
        # Note that ImageFolder takes the root directory path, NOT the subdirectory path for a specific split.
        self.train_dataset = ImageFolder(root=os.path.join(self.data_dir, 'train'), transform=train_transforms)
        self.val_dataset = ImageFolder(root=os.path.join(self.data_dir, 'val'), transform=val_transforms)
        self.test_dataset = ImageFolder(root=os.path.join(self.data_dir, 'test'), transform=val_transforms)

        self.num_classes = len(self.train_dataset.classes)  # Dynamically detects the number of classes

        print(f"Datasets loaded. Train: {len(self.train_dataset)} images, Val: {len(self.val_dataset)} images, Test: {len(self.test_dataset)} images.")
        print(f"Detected classes: {self.train_dataset.classes}")

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size,
                          shuffle=True, num_workers=self.num_workers, pin_memory=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size,
                          shuffle=False, num_workers=self.num_workers, pin_memory=True)

    def test_dataloader(self):
        return DataLoader(self.test_dataset, batch_size=self.batch_size,
                          shuffle=False, num_workers=self.num_workers, pin_memory=True)

# %%66666666666666
# Redefinition of WNBModule with paths pointing to Google Drive

# ... (keep your ASLDataModule as-is, just above in the same cell) ...

class WNBModule(LightningModule):
    def __init__(self, project_name: str, experiment_name: str,
                 checkpoint_dir: str,  # This path is now on Drive
                 wandb_save_dir: str,  # New parameter for the W&B logs folder
                 time_interval: int = 15):
        super().__init__()
        self.project_name = project_name
        self.experiment_name = experiment_name
        self.checkpoint_dir = checkpoint_dir
        self.wandb_save_dir = wandb_save_dir  # Stores the path to the W&B logs
        self.time_interval = time_interval

        # Make sure the checkpoint directory exists on Drive
        os.makedirs(self.checkpoint_dir, exist_ok=True)

        # Initialize W&B logger
        # 'save_dir' now points to Google Drive
        self.wandb_logger = WandbLogger(
            project=self.project_name,
            name=self.experiment_name,
            save_dir=self.wandb_save_dir,  # The path to Drive for W&B logs
            resume="allow"
        )
        print(f"W&B Logger initialized for project: {self.wandb_logger.experiment.project_name}, Run: {self.wandb_logger.experiment.name}")

        # Define callbacks (no major changes here, they will use self.checkpoint_dir)
        self.callbacks_list = []

        # Checkpoint per epoch
        self.ckpt_epoch = ModelCheckpoint(
            dirpath=self.checkpoint_dir,  # Path on Drive
            filename="epoch-{epoch:02d}-{val_acc:.3f}",
            save_top_k=-1,
            every_n_epochs=1,
            monitor='val_acc',
            mode='max',
            save_last=True
        )
        self.callbacks_list.append(self.ckpt_epoch)

        # Time-based checkpoint
        class TimeBasedCheckpoint(Callback):
            def __init__(self, save_every_minutes, dirpath_base):
                super().__init__()
                self.save_every_sec = save_every_minutes * 60
                self.dirpath_base = dirpath_base
                self.last_save = time.time()

            def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
                current_time = time.time()
                if current_time - self.last_save >= self.save_every_sec:
                    ckpt_filename = f"time_ckpt_epoch{trainer.current_epoch:02d}_step{trainer.global_step}_{int(current_time)}.ckpt"
                    ckpt_path = os.path.join(self.dirpath_base, ckpt_filename)
                    trainer.save_checkpoint(ckpt_path)
                    pl_module.print(f"Saved a time-based checkpoint: {ckpt_path}")
                    self.last_save = current_time

        self.ckpt_time = TimeBasedCheckpoint(self.time_interval, self.checkpoint_dir)  # Path on Drive
        self.callbacks_list.append(self.ckpt_time)

        # Early Stopping
        self.early_stop_callback = EarlyStopping(
            monitor='val_acc',
            patience=7,
            mode='max',
            verbose=True
        )
        self.callbacks_list.append(self.early_stop_callback)

    def train_model(self, datamodule: pl.LightningDataModule, max_epochs: int, gpus: int = 1, **trainer_kwargs):
        """
        Starts the model training, handling checkpoint resumption.
        """
        # Detect the latest checkpoint for resumption (now on Drive)
        all_ckpts = glob.glob(os.path.join(self.checkpoint_dir, "*.ckpt"))
        ckpt_path = None
        if all_ckpts:
            ckpt_path = max(all_ckpts, key=os.path.getmtime)
            print(f"Resuming training from checkpoint: {ckpt_path}")
        else:
            print("No checkpoint found on Drive, starting a new training session.")

        # Trainer configuration
        trainer = Trainer(
            logger=self.wandb_logger,
            callbacks=self.callbacks_list,
            max_epochs=max_epochs,
            accelerator="gpu" if gpus > 0 else "cpu",
            devices=gpus if gpus > 0 else 1,
            precision=16,
            log_every_n_steps=50,
            deterministic=True,
            **trainer_kwargs
        )

        # Start training
        print("\n--- Starting training ---")
        trainer.fit(self, datamodule=datamodule, ckpt_path=ckpt_path)
        print("--- Training complete ---")

        # Run test on the best saved model
        print("\n--- Starting test on the best model ---")
        trainer.test(datamodule=datamodule, ckpt_path='best')
        print("--- Testing complete ---")

        # Finish W&B run
        wandb.finish()

# %%7
# Definition of your specific model (ASLClassifier) that inherits from WNBModule

import torch
import torch.nn as nn
import torchvision.models as models
import pytorch_lightning as pl  # Ensure PyTorch Lightning is imported
from torchmetrics import Accuracy  # Ensure torchmetrics.Accuracy is imported
from typing import Any  # For type annotations (makes code more readable and robust)

class ASLClassifier(WNBModule):
    def __init__(self, num_classes: int, lr: float = 3e-4, weight_decay: float = 1e-4,
                 initial_freeze_epochs: int = 5,
                 # These arguments are passed directly to WNBModule’s constructor.
                 # They have default values but can be customized when instantiating.
                 project_name: str = "ASL-Alphabet-Classifier-Drive",
                 experiment_name: str = "ResNet18-FineTune-Default",  # A default name if not specified
                 checkpoint_dir: str = "/content/drive/My Drive/ASL_Project_WNB/checkpoints",  # Default path
                 wandb_save_dir: str = "/content/drive/My Drive/ASL_Project_WNB/wandb_logs",  # Default path
                 time_interval: int = 15):

        # Call the constructor of the parent class (`WNBModule`).
        # This line initializes the `WandbLogger` and configures checkpointing `callbacks`.
        super().__init__(
            project_name=project_name,
            experiment_name=experiment_name,
            checkpoint_dir=checkpoint_dir,
            wandb_save_dir=wandb_save_dir,
            time_interval=time_interval
        )

        # Save hyperparameters specific to `ASLClassifier`.
        # The parameters of `WNBModule` are already saved via its own `super().__init__`.
        # Paths and names already handled by the parent class are ignored.
        self.save_hyperparameters(ignore=['project_name', 'experiment_name', 'checkpoint_dir', 'wandb_save_dir', 'time_interval'])

        # --- Model architecture ---
        # 1. Load pre-trained `ResNet18` on ImageNet.
        # `weights=models.ResNet18_Weights.DEFAULT` downloads pretrained weights,
        # which is a great starting point for computer vision tasks.
        self.backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)

        # 2. Freeze pretrained layers of the backbone for initial training.
        # This means their weights won’t be updated initially.
        # They’ll be unfrozen later for fine-tuning if `initial_freeze_epochs` > 0.
        for param in self.backbone.parameters():
            param.requires_grad = False  # Prevent gradient computation and weight updates.

        # 3. Replace the final fully connected layer (classification "head").
        # `ResNet18` outputs 2048 features before the final layer.
        # Replace it with a new layer producing `num_classes` outputs (one per ASL sign).
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_features, num_classes)  # New output layer for ASL classes.

        # --- Performance metrics ---
        # Used to track accuracy on train, validation, and test sets.
        # `torchmetrics.Accuracy` handles averaging and aggregation.
        self.train_acc = Accuracy(task="multiclass", num_classes=num_classes)
        self.val_acc = Accuracy(task="multiclass", num_classes=num_classes)
        self.test_acc = Accuracy(task="multiclass", num_classes=num_classes)

        # --- Loss function ---
        # `nn.CrossEntropyLoss` is the standard loss for multi-class classification.
        # `label_smoothing=0.1` is a regularization technique that softens label targets,
        # helping prevent overfitting and improving model confidence calibration.
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Defines the model's forward pass.
        This is how the input data (`x`) flows through the network to produce output.
        """
        return self.backbone(x)  # Forward pass through the modified ResNet18

    def configure_optimizers(self):
        """
        Configures the optimizer and learning rate scheduler.
        These control how model weights are updated during training.
        """
        # `AdamW` optimizer: a variant of Adam that handles L2 regularization (weight decay) better,
        # improving generalization.
        optimizer = torch.optim.AdamW(
            self.parameters(),  # Optimizes parameters with `requires_grad=True`
            lr=self.hparams.lr,  # Learning rate from saved hyperparameters
            weight_decay=self.hparams.weight_decay,  # L2 regularization weight
        )

        # CosineAnnealingLR: learning rate follows a cosine curve,
        # starts high, decays, and then increases again — helps exploration.
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.trainer.max_epochs)

        return {
            'optimizer': optimizer,
            'lr_scheduler': {
                'scheduler': scheduler,
                'interval': 'epoch',  # Scheduler updates every epoch
                'frequency': 1
            }
        }

    def on_train_epoch_start(self):
        """
        Hook called at the start of each training epoch.
        Used here for progressive unfreezing of backbone layers.
        """
        # Unfreeze backbone layers after a few initial training epochs.
        # This first trains the new classifier head, then fine-tunes deeper pretrained layers.
        if self.current_epoch == self.hparams.initial_freeze_epochs and self.hparams.initial_freeze_epochs > 0:
            print(f"\n--- Unfreezing backbone layers at epoch {self.current_epoch}. Starting fine-tuning ---")
            for name, param in self.backbone.named_parameters():
                if 'layer4' in name or 'layer3' in name:  # Targets deeper convolutional blocks of ResNet
                    param.requires_grad = True  # Enable gradients for these parameters

            # Note: For finer control, you could assign different learning rates to
            # newly unfrozen layers and the classifier head using parameter groups.

    def training_step(self, batch: Any, batch_idx: int):
        """
        Defines the logic for a single training step (one batch).
        """
        images, targets = batch  # Get images and their target labels
        logits = self(images)  # Forward pass to get predictions
        loss = self.criterion(logits, targets)  # Compute loss

        preds = torch.argmax(logits, dim=1)  # Get predicted class indices
        self.train_acc.update(preds, targets)  # Update accuracy metric

        # Log training loss and accuracy
        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        return loss  # Used for backpropagation and optimization

    def on_train_epoch_end(self):
        """
        Hook called at the end of each training epoch.
        """
        # With `on_epoch=True`, TorchMetrics resets metrics automatically at the end of the epoch.
        pass

    def validation_step(self, batch: Any, batch_idx: int):
        """
        Defines the logic for a single validation step.
        Similar to `training_step`, but without gradient computation or weight updates.
        """
        images, targets = batch
        logits = self(images)
        loss = self.criterion(logits, targets)

        preds = torch.argmax(logits, dim=1)
        self.val_acc.update(preds, targets)

        # Log validation loss and accuracy.
        # These metrics help detect overfitting (e.g., if `val_loss` increases while `train_loss` decreases).
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)

    def on_validation_epoch_end(self):
        """
        Hook called at the end of each validation epoch.
        """
        pass  # Metric reset handled automatically

    def test_step(self, batch: Any, batch_idx: int):
        """
        Defines the logic for a single test step.
        Evaluated once after training on unseen data.
        """
        images, targets = batch
        logits = self(images)
        loss = self.criterion(logits, targets)

        preds = torch.argmax(logits, dim=1)
        self.test_acc.update(preds, targets)

        # Log test loss and accuracy
        # These metrics provide a final, unbiased evaluation of model performance.
        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log('test_acc', self.test_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)

    def on_test_epoch_end(self):
        """
        Hook called at the end of the test phase.
        """
        pass  # Metric reset handled automatically

# %%8
# Launching training with Google Drive paths

# 1. Initialize the DataModule
# The raw and split data remain in /content/
data_module = ASLDataModule(
    data_dir='/content/data/',
    batch_size=64,
    num_workers=4
)
data_module.setup()

# 2. Initialize the ASLClassifier model
# All W&B and checkpointing parameters are handled by WNBModule
model = ASLClassifier(
    num_classes=data_module.num_classes,
    lr=3e-4,
    weight_decay=1e-4,
    initial_freeze_epochs=5,
    project_name="ASL-Alphabet-Classifier-Drive",  # You can change the project name to differentiate
    experiment_name="ResNet18-Drive-Run-1",
    checkpoint_dir=checkpoints_dir_drive,  # NEW GOOGLE DRIVE PATH FOR CHECKPOINTS
    wandb_save_dir=wandb_logs_dir_drive,   # NEW GOOGLE DRIVE PATH FOR W&B LOGS
    time_interval=15
)

# 3. Start training by calling the train_model() method from WNBModule
model.train_model(
    datamodule=data_module,
    max_epochs=50,
    gpus=1
)

print("\nTraining and testing complete.")
print(f"Your checkpoints are located on Google Drive in: {checkpoints_dir_drive}")
print(f"Your W&B logs are located on Google Drive in: {wandb_logs_dir_drive}")

##9
# Cell where you mount Google Drive and define the paths
from google.colab import drive
import os  # Make sure 'os' is imported

# drive.mount('/content/drive')  # Uncomment this to mount Google Drive

# --- Define base paths ---
DRIVE_PATH = "/content/drive/My Drive"  # Base path to your Google Drive

# Path to your ASL project folder
ASL_PROJECT_ROOT = os.path.join(DRIVE_PATH, "ASL_Project_WNB")

# Path where your unzipped data is located (usually in Colab)
DEST_ROOT = "/content/data"

# --- Define path variables used by WNBModule and for export ---
PROJECT_NAME = "ASL-Alphabet-Classifier-Drive"
EXPERIMENT_NAME = "ResNet18-Drive-Run-1"  # Name of your W&B training session
CHECKPOINT_DIR = os.path.join(ASL_PROJECT_ROOT, "checkpoints")  # Folder where checkpoints are saved
WANDB_SAVE_DIR = os.path.join(ASL_PROJECT_ROOT, "wandb_logs")  # Folder for local W&B logs

# Create the folders if they don’t exist (just in case)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(WANDB_SAVE_DIR, exist_ok=True)
print("Project paths set and directories created.")

# %%10
print("\n--- Visualizing predictions ---")

# 1. Load the best model
# The path to the best checkpoint is the one found by the Trainer or the best file path
# In your previous log, it was: /content/drive/My Drive/ASL_Project_WNB/checkpoints/epoch-epoch=14-val_acc=1.000.ckpt
# Replace "YOUR_BEST_MODEL_CHECKPOINT_PATH" with the exact path of the best checkpoint
# This path is also shown in the test log: INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at /content/drive/My Drive/ASL_Project_WNB/checkpoints/epoch-epoch=14-val_acc=1.000.ckpt
best_model_path = os.path.join(CHECKPOINT_DIR, "epoch-epoch=13-val_acc=1.000.ckpt")

checkpoint = torch.load(best_model_path, map_location='cpu')
checkpoint

model.load_state_dict(checkpoint['state_dict'], strict=False)

model.eval()

checkpoint = torch.load(best_model_path, map_location='cpu')
state_dict = checkpoint['state_dict']

model.load_state_dict(state_dict, strict=False)

# # PyTorch Lightning provides a specific function to load a model from a checkpoint,
# # which automatically reconstructs the model and loads stored hyperparameters.
# model_for_inference = WNBModule.load_from_checkpoint(
#     best_model_path,
#     # These arguments are passed to WNBModule’s __init__ during model reconstruction.
#     # Since save_hyperparameters() is used, Lightning can recover them, but specifying them doesn’t hurt.
#     project_name=PROJECT_NAME,
#     experiment_name=EXPERIMENT_NAME,
#     checkpoint_dir=CHECKPOINT_DIR,
#     wandb_save_dir=WANDB_SAVE_DIR,
#     num_classes=29,
#     learning_rate=1e-4,
#     unfreeze_epoch=5,
#     time_interval=15
# )

# # Set the model to evaluation mode (very important for inference)
# model_for_inference.eval()

# If a GPU is available, move the model to GPU
if torch.cuda.is_available():
    model.cuda()
    print("Model moved to GPU for inference.")
else:
    print("No GPU available. Model is on CPU for inference.")

# 2. Prepare the DataLoader for the test set
# Reuse the same transforms as used for training/testing
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Create the test dataset from the location where your images are extracted
TEST_DATA_DIR = os.path.join(DEST_ROOT, 'test')  # /content/data/test

# Use ImageFolder if the DataModule isn’t easily accessible here,
# or simply for convenience.
test_dataset = datasets.ImageFolder(root=TEST_DATA_DIR, transform=transform)

# Get class names in correct order
class_names = test_dataset.classes
print(f"Test dataset classes: {class_names}")

test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=2)

# 3. Fetch one batch of images and make predictions
dataiter = iter(test_loader)
images, labels = next(dataiter)

# If GPU is available, move images to GPU
if torch.cuda.is_available():
    images = images.cuda()

# Make predictions (disable gradient calculation)
with torch.no_grad():
    outputs = model(images)
    _, predicted = torch.max(outputs, 1)  # Get class with highest probability

# Move images and labels back to CPU for display
images = images.cpu()
labels = labels.cpu()
predicted = predicted.cpu()

# 4. Display images with true labels and predictions
fig = plt.figure(figsize=(12, 10))  # Adjust size if needed
for idx in np.arange(len(images)):
    ax = fig.add_subplot(1, len(images), idx + 1, xticks=[], yticks=[])

    # Unnormalize the image for proper display
    image = images[idx].numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    image = std * image + mean
    image = np.clip(image, 0, 1)

    ax.imshow(image)
    ax.set_title(f"Predicted: {class_names[predicted[idx]]}\nTrue: {class_names[labels[idx]]}",
                 color=("green" if predicted[idx] == labels[idx] else "red"))
plt.tight_layout()
plt.show()

print("Prediction visualization complete.")

# %%11
print("\n--- Exporting the model for future use ---")

# Path to the best checkpoint (same one used for visualization)
best_model_path = os.path.join(CHECKPOINT_DIR, "epoch-epoch=13-val_acc=1.000.ckpt")

# # Load the model from the checkpoint (PyTorch Lightning handles architecture and weights)
# model = WNBModule.load_from_checkpoint(
#     best_model_path,
#     project_name=PROJECT_NAME,
#     experiment_name=EXPERIMENT_NAME,
#     checkpoint_dir=CHECKPOINT_DIR,
#     wandb_save_dir=WANDB_SAVE_DIR,
#     num_classes=29,
#     learning_rate=1e-3,
#     unfreeze_epoch=5,
#     time_interval=15
# )

# Set model to evaluation mode (important for inference)
model.eval()

# Move the model to CPU for saving (.pth files are often best saved from CPU)
model.to('cpu')

# Define the path where the exported model will be saved on Google Drive
export_model_dir = os.path.join(DRIVE_PATH, "ASL_Project_WNB", "exported_model")
os.makedirs(export_model_dir, exist_ok=True)
# exported_model_path = os.path.join(export_model_dir, "asl_resnet18_best_model.pth")
exported_model_path = os.path.join(export_model_dir, "asl_resnet18_best_model.pt")


# Save only the model's state (its weights)
# This is the most flexible method for reloading the model later
torch.save(model.state_dict(), exported_model_path)

print(f"Model exported and saved to: {exported_model_path}")
print("\nTo load this model later in another script or environment:")
print("  import torch")
print("  from your_model_file import WNBModule  # Or redefine the WNBModule class")
print("  model_loaded = WNBModule(num_classes=29, learning_rate=1e-3, unfreeze_epoch=5, ...)")
print(f"  model_loaded.load_state_dict(torch.load('{exported_model_path}', map_location=torch.device('cpu')))")
print("  model_loaded.eval()")

import os
import torch
import torchvision.transforms as transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report
from tqdm import tqdm

# --- Setup ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.eval()
model.to(device)

# --- Transforms ---
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# --- Dataset ---
TEST_DATA_DIR = os.path.join(DEST_ROOT, 'test')  # Make sure DEST_ROOT is defined
test_dataset = datasets.ImageFolder(root=TEST_DATA_DIR, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)
class_names = test_dataset.classes
print(f"Class names: {class_names}")

# --- Inference ---
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in tqdm(test_loader, desc="Evaluating"):
        images = images.to(device)
        labels = labels.to(device)

        outputs = model(images)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# --- Confusion Matrix ---
cm = confusion_matrix(all_labels, all_preds)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)

fig, ax = plt.subplots(figsize=(12, 12))
disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)
plt.title("ASL Test Set Confusion Matrix")
plt.show()

# --- Per-Class Accuracy ---
all_preds_np = np.array(all_preds)
all_labels_np = np.array(all_labels)

print("\n📊 Per-Class Accuracy:")
for idx, class_name in enumerate(class_names):
    class_mask = all_labels_np == idx
    correct = (all_preds_np[class_mask] == idx).sum()
    total = class_mask.sum()
    accuracy = correct / total * 100 if total > 0 else 0.0
    print(f"{class_name:>10}: {accuracy:.2f}%")

# --- Classification Report (Optional: includes precision/recall/F1) ---
print("\n🧾 Classification Report:")
print(classification_report(all_labels, all_preds, target_names=class_names, digits=2))

# #--- Exportation du modèle pour utilisation future ---
# #/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
# #W&B Logger initialisé pour le projet: <bound method Run.project_name of <wandb.sdk.wandb_run.Run object at 0x780421f72fd0>>, Run: ResNet18-Drive-Run-1
# #Modèle exporté et sauvegardé à : /content/drive/My Drive/ASL_Project_WNB/exported_model/asl_resnet18_best_model.pth

# To load this model later in another script or environment:
import torch
from your_model_file import WNBModule  # Or redefine the WNBModule class
model_loaded = WNBModule(num_classes=29, learning_rate=1e-3, unfreeze_epoch=5, ...)
model_loaded.load_state_dict(torch.load('/content/drive/My Drive/ASL_Project_WNB/exported_model/asl_resnet18_best_model.pt', map_location=torch.device('cpu')))
model_loaded.eval()